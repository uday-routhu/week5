{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxImdPJYHxNKGhTNuqvC1t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uday-routhu/week5/blob/master/Project_2_Part_1_Core_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Project 2 - Part 1 (Core):"
      ],
      "metadata": {
        "id": "a7BdlLJn-VBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Author: Udayakumar Routhu"
      ],
      "metadata": {
        "id": "AQXN8twM-dGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First choice: dataset 1"
      ],
      "metadata": {
        "id": "eXcc-17sKgXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Source of data\n",
        "\n",
        "2. Brief description of data\n",
        "\n",
        "3. What is the target?\n",
        "\n",
        "4. What does one row represent? (A person? A business? An event? A product?)\n",
        "\n",
        "5. Is this a classification or regression problem?\n",
        "\n",
        "6. How many features does the data have?\n",
        "\n",
        "7. How many rows are in the dataset?\n",
        "\n",
        "8. What, if any, challenges do you foresee in cleaning, exploring, or modeling this dataset?\n"
      ],
      "metadata": {
        "id": "BG7H2Qo_-oRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "pd.set_option('display.max_columns',100)\n",
        "\n",
        "import missingno as msno\n",
        "## Setting the max_columns to 50\n",
        "pd.set_option('display.max_columns',50)\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Models\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Classification Metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, \\\n",
        "f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn import set_config\n",
        "set_config(transform_output='pandas')\n",
        "\n",
        "# Set global scikit-learn configuration\n",
        "from sklearn import set_config\n",
        "# Display estimators as a diagram\n",
        "set_config(display='diagram') # '"
      ],
      "metadata": {
        "id": "u8kMFgdg-1vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tpE7fOd1--qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define EDA Functions"
      ],
      "metadata": {
        "id": "XSFsjxurTVoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###explore_categorical\n"
      ],
      "metadata": {
        "id": "kzj1pxFaT2OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def explore_categorical_check_constant(df, x, fillna = True, placeholder = 'MISSING',\n",
        "def explore_categorical(df, x, fillna = True, placeholder = 'MISSING',\n",
        "                        figsize = (6,4), order = None):\n",
        "  \"\"\"BEST VERSION\"\"\"\n",
        "  # Make a copy of the dataframe and fillna\n",
        "  temp_df = df.copy()\n",
        "\n",
        "\n",
        "  ## Save null value counts and percent for printing\n",
        "  null_count = temp_df[x].isna().sum()\n",
        "  null_perc = null_count/len(temp_df)* 100\n",
        "\n",
        "\n",
        "  # fillna with placeholder\n",
        "  if fillna == True:\n",
        "    temp_df[x] = temp_df[x].fillna(placeholder)\n",
        "\n",
        "\n",
        "  # Create figure with desired figsize\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  ## Plotting a count plot\n",
        "  sns.countplot(data=temp_df, x=x, ax=ax, order=order)\n",
        "\n",
        "  # Rotate Tick Labels for long names\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "  # Add. atitle with the feature name included\n",
        "  ax.set_title(f\"Column: {x}\", fontweight='bold')\n",
        "\n",
        "  # Fix layout and show plot (before print statements)\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # Print null value info\n",
        "  print(f\"- NaN's Found: {null_count} ({round(null_perc,2)}%)\")\n",
        "  # Print cardinality info\n",
        "  nunique = temp_df[x].nunique()\n",
        "  print(f\"- Unique Values: {nunique}\")\n",
        "\n",
        "\n",
        "  # Get the most most common value, its count as # and as %\n",
        "  most_common_val_count = temp_df[x].value_counts(dropna=False).head(1)\n",
        "  most_common_val = most_common_val_count.index[0]\n",
        "  freq = most_common_val_count.values[0]\n",
        "\n",
        "  perc_most_common = freq / len(temp_df) * 100\n",
        "  print(f\"- Most common value: '{most_common_val}' occurs {freq} times ({round(perc_most_common,2)}%)\")\n",
        "\n",
        "  # print message if quasi-constant or constant (most common val more than 98% of data)\n",
        "  if perc_most_common > 98:\n",
        "    print(f\"\\n- [!] Warning: '{x}' is a constant or quasi-constant feature and should be dropped.\")\n",
        "\n",
        "  return fig, ax"
      ],
      "metadata": {
        "id": "ZWG2Wf-pTZiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_categorical_vs_target(df, x, y,figsize=(6,4),\n",
        "                            fillna = True, placeholder = 'MISSING',\n",
        "                            order = None):\n",
        "  # Make a copy of the dataframe and fillna\n",
        "  temp_df = df.copy()\n",
        "  # # Save null values before imputing\n",
        "  # null_count = temp_df[x].isna().sum()\n",
        "  # null_perc = null_count/len(df)* 100\n",
        "\n",
        "  ## fillna with placeholder\n",
        "  if fillna == True:\n",
        "    temp_df[x] = temp_df[x].fillna(placeholder)\n",
        "\n",
        "  # or drop nulls prevent unwanted 'nan' group in stripplot\n",
        "  else:\n",
        "    temp_df = temp_df.dropna(subset=[x])\n",
        "\n",
        "\n",
        "  ## Create the figure and subplots\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Barplot\n",
        "  sns.barplot(data=temp_df, x=x, y=y, ax=ax, order=order, alpha=0.6,\n",
        "              linewidth=1, edgecolor='black', errorbar=None)\n",
        "\n",
        "  # Boxplot\n",
        "  sns.stripplot(data=temp_df, x=x, y=y, hue=x, ax=ax,\n",
        "                order=order, hue_order=order, legend=False,\n",
        "                edgecolor='white', linewidth=0.5,\n",
        "                size=3,zorder=0)\n",
        "\n",
        "  # Rotate xlabels\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "  # Add a title\n",
        "  ax.set_title(f\"{x} vs. {y}\", fontweight='bold')\n",
        "  fig.tight_layout()\n",
        "    # show fig and print\n",
        "  plt.show()\n",
        "  # print(f\"- NaN's Found: {null_count} ({round(null_perc,2)}%)\")\n",
        "\n",
        "  return fig, ax"
      ],
      "metadata": {
        "id": "5-FJun4hTf3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###explore_numeric"
      ],
      "metadata": {
        "id": "4hOtjfP-T8Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: add the new print statements from explore_categorical\n",
        "def explore_numeric(df, x, figsize=(6,5) ):\n",
        "\n",
        "  ## Save null value counts and percent for printing\n",
        "  null_count = df[x].isna().sum()\n",
        "  null_perc = null_count/len(df)* 100\n",
        "\n",
        "\n",
        "  ## Making our figure with gridspec for subplots\n",
        "  gridspec = {'height_ratios':[0.7,0.3]}\n",
        "  fig, axes = plt.subplots(nrows=2, figsize=figsize,\n",
        "                           sharex=True, gridspec_kw=gridspec)\n",
        "  # Histogram on Top\n",
        "  sns.histplot(data=df, x=x, ax=axes[0])\n",
        "\n",
        "  # Boxplot on Bottom\n",
        "  sns.boxplot(data=df, x=x, ax=axes[1])\n",
        "\n",
        "  ## Adding a title\n",
        "  axes[0].set_title(f\"Column: {x}\", fontweight='bold')\n",
        "\n",
        "  ## Adjusting subplots to best fill Figure\n",
        "  fig.tight_layout()\n",
        "\n",
        "  # Ensure plot is shown before message\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # Print null value info\n",
        "  print(f\"- NaN's Found: {null_count} ({round(null_perc,2)}%)\")\n",
        "  # Print cardinality info\n",
        "  nunique = df[x].nunique()\n",
        "  print(f\"- Unique Values: {nunique}\")\n",
        "\n",
        "\n",
        "  # Get the most most common value, its count as # and as %\n",
        "  most_common_val_count = df[x].value_counts(dropna=False).head(1)\n",
        "\n",
        "  most_common_val = most_common_val_count.index[0]\n",
        "  freq = most_common_val_count.values[0]\n",
        "  perc_most_common = freq / len(df) * 100\n",
        "\n",
        "  print(f\"- Most common value: '{most_common_val}' occurs {freq} times ({round(perc_most_common,2)}%)\")\n",
        "\n",
        "  # print message if quasi-constant or constant (most common val more than 98% of data)\n",
        "  if perc_most_common > 98:\n",
        "    print(f\"\\n- [!] Warning: '{x}' is a constant or quasi-constant feature and should be dropped.\")\n",
        "\n",
        "  return fig, axes"
      ],
      "metadata": {
        "id": "7OxSg63nTp-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_numeric_vs_target(df, x, y,figsize=(6,4),):\n",
        "  # Calculate the correlation\n",
        "  corr = df[[x,y]].corr().round(2)\n",
        "  r = corr.loc[x,y]\n",
        "\n",
        "  # Plot the data\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  scatter_kws={'ec':'white','lw':1,'alpha':0.8}\n",
        "  sns.regplot(data=df, x=x, y=y, ax=ax, scatter_kws=scatter_kws)\n",
        "\n",
        "  ## Add the title with the correlation\n",
        "  ax.set_title(f\"{x} vs. {y} (r = {r})\", fontweight='bold')\n",
        "\n",
        "  ## Print message with info on the count and % of null values\n",
        "  null_count = df[x].isna().sum()\n",
        "  if null_count > 0:\n",
        "    null_perc = null_count/len(df)* 100\n",
        "    print(f\"- NaN's Found: {null_count} ({round(null_perc,2)}%)\")\n",
        "\n",
        "  return fig, ax"
      ],
      "metadata": {
        "id": "XQDGUazXTwgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load in the dataset"
      ],
      "metadata": {
        "id": "VyaijDBcAZlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data set\n",
        "fpath_adult = \"/content/drive/MyDrive/CodingDojo/02-MachineLearning/Week08/Data/adult.csv\"\n",
        "df_adult = pd.read_csv(fpath_adult)"
      ],
      "metadata": {
        "id": "8GEd2u2r_Htb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_adult.info()\n",
        "df_adult.head()"
      ],
      "metadata": {
        "id": "U-vPEtluNY5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Source of data\n",
        "  The data can be downloaded from [this link](https://www.kaggle.com/datasets/wenruliu/adult-income-dataset)"
      ],
      "metadata": {
        "id": "JBkr9jEfZUkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Brief description of data\n",
        "- An individual’s annual income results from various factors. Intuitively, it is influenced by the individual’s education level, age, gender, occupation, and etc."
      ],
      "metadata": {
        "id": "7l6wUXz5ZUlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###What is the target?\n",
        "- Income"
      ],
      "metadata": {
        "id": "vjmj18GwZUlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###What does one row represent? (A person? A business? An event? A product?)\n",
        "- A person"
      ],
      "metadata": {
        "id": "bNGUzDMaZUlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Is this a classification or regression problem?\n",
        " - Since the goal is to predict whether an individual's income is above or below a threshold (categorical outcome), this is a classification problem."
      ],
      "metadata": {
        "id": "_gdR6BWQZUlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How many features does the data have?  \n",
        " - Number of attributes: 14\n",
        " - These are the demographics and other features to describe a person"
      ],
      "metadata": {
        "id": "LABx_yHzZUlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How many rows are in the dataset?\n",
        "- Number of rows: 48842"
      ],
      "metadata": {
        "id": "KZXosMWmZUlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###What, if any, challenges do you foresee in cleaning, exploring, or modeling this dataset?\n",
        " - we clean and explore the data before running the models"
      ],
      "metadata": {
        "id": "qAP0AehVZUlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CLean and Prepare The Data"
      ],
      "metadata": {
        "id": "NR0b_AWMfw0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Identified and addressed missing values."
      ],
      "metadata": {
        "id": "kS1QErR_f8Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_adult.isna().sum()"
      ],
      "metadata": {
        "id": "MkuFzKumgArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are no null values to impute place holder"
      ],
      "metadata": {
        "id": "g-eJ3WOsgMGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Deleted duplicate rows."
      ],
      "metadata": {
        "id": "y0SHAafSgkef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Explore Data\n",
        "df_adult.duplicated().sum()"
      ],
      "metadata": {
        "id": "pKTBs7U6gblC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are 52 duplicated records in dataset"
      ],
      "metadata": {
        "id": "ZTyZTQiqguRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop duplicated record\n",
        "df_adult = df_adult.drop_duplicates()"
      ],
      "metadata": {
        "id": "1R9R2gvwg5dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#again check duplicates dropped or not\n",
        "df_adult.duplicated().sum()"
      ],
      "metadata": {
        "id": "0E7uI1Zpg-7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* now, there are no duplicated rows"
      ],
      "metadata": {
        "id": "VtVrWIP-hDK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check for impossible numeric values/Delete unnecessary columns."
      ],
      "metadata": {
        "id": "94i8_askhccR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_adult.info()"
      ],
      "metadata": {
        "id": "pwicxy3phiVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* there is no data type missing"
      ],
      "metadata": {
        "id": "bUqSht5Oh78e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Identified and corrected inconsistencies in data for categorical values (i.e. Cat, cat, cats)."
      ],
      "metadata": {
        "id": "oLDKGvmkiEjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_types = df_adult.dtypes\n",
        "str_cols = data_types[data_types=='object'].index\n",
        "for col in str_cols:\n",
        "    print(f'- {col}:')\n",
        "    print(df_adult[col].value_counts(dropna=False))\n",
        "    print(\"\\n\\n\")\n",
        "    print(df_adult[col])"
      ],
      "metadata": {
        "id": "iRyKLNlAiLrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There is specail char '?' availble in 'relationship','workclass' column so removing that value"
      ],
      "metadata": {
        "id": "KLdRypRJinlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values_to_remove = ['?']\n",
        "# Remove rows containing specified values in the 'year' column\n",
        "df_adult = df_adult[~df_adult['relationship'].isin(values_to_remove)]\n",
        "df_adult = df_adult[~df_adult['workclass'].isin(values_to_remove)]"
      ],
      "metadata": {
        "id": "GA4u40nag78I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_types = df_adult.dtypes\n",
        "str_cols = data_types[data_types=='object'].index\n",
        "for col in str_cols:\n",
        "    print(f'- {col}:')\n",
        "    print(df_adult[col].value_counts(dropna=False))\n",
        "    print(\"\\n\\n\")\n",
        "    print(df_adult[col])"
      ],
      "metadata": {
        "id": "HzS294pTiThM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now , there are no inconsistency columns"
      ],
      "metadata": {
        "id": "T-SUSsC0izN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check for and address any impossible values in numeric columns."
      ],
      "metadata": {
        "id": "Hs1bJ8aAirGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats =  df_adult.describe()\n",
        "stats.loc[['mean','min','max']]"
      ],
      "metadata": {
        "id": "IO9f-pP9i6i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Outliers"
      ],
      "metadata": {
        "id": "OojcKxIEC8FY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are no impossible numeric values\n",
        "* no extreme outliers are noted"
      ],
      "metadata": {
        "id": "rM_4d-mqjYO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_adult.describe(include='object')"
      ],
      "metadata": {
        "id": "m_DQhVwTDhQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Explore the data"
      ],
      "metadata": {
        "id": "MgDD1joSmtwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define EDA Functions"
      ],
      "metadata": {
        "id": "7GLzJ4zqmzFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####workclass"
      ],
      "metadata": {
        "id": "yirdGWfZK-ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirming the dtype to select correct EDA function\n",
        "feature = \"workclass\"\n",
        "df_adult[feature].dtype"
      ],
      "metadata": {
        "id": "KUftkblHKufA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_adult.info()"
      ],
      "metadata": {
        "id": "LeTNRMFhLV1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = explore_categorical(df_adult, \"workclass\", figsize=(5,3))"
      ],
      "metadata": {
        "id": "XKxgdvfbpDgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations\n",
        "\n",
        "1. What type of feature is it? (Nominal category, ordinal, numeric)\n",
        "  - Categorical (nominal)\n",
        "\n",
        "2. How many null values? What percentage? What would you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No, the most common category makes up only 73.62% of the feature.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - There are 8 categories. Not high cardinality ( < 10 categories)\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No.\n"
      ],
      "metadata": {
        "id": "0aOsDFpYzWKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####education"
      ],
      "metadata": {
        "id": "1ok1HP1RM8pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = explore_categorical(df_adult, \"education\", figsize=(5,3))"
      ],
      "metadata": {
        "id": "iTKk-GloMz1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations\n",
        "\n",
        "1. What type of feature is it? (Nominal category, ordinal, numeric)\n",
        "  - Categorical (nominal)\n",
        "\n",
        "2. How many null values? What percentage? What would you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No, the most common category makes up only 32.53% of the feature.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - There are 16 categories. high cardinality ( > 10 categories)\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No."
      ],
      "metadata": {
        "id": "PlkU3fxENCAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####marital-status"
      ],
      "metadata": {
        "id": "DmoHoMLONVB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = explore_categorical(df_adult, \"marital-status\", figsize=(5,3))"
      ],
      "metadata": {
        "id": "u9ZeabOONYOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations\n",
        "\n",
        "1. What type of feature is it? (Nominal category, ordinal, numeric)\n",
        "  - Categorical (nominal)\n",
        "\n",
        "2. How many null values? What percentage? What would you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No, the most common category makes up only 46.61% of the feature.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - There are 7 categories.No high cardinality ( < 10 categories)\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No."
      ],
      "metadata": {
        "id": "c2MCra5KNfpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####occupation"
      ],
      "metadata": {
        "id": "qJi4-Rc4NuhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = explore_categorical(df_adult, \"occupation\", figsize=(5,3))"
      ],
      "metadata": {
        "id": "5Xl4oUDiNzEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations\n",
        "\n",
        "1. What type of feature is it? (Nominal category, ordinal, numeric)\n",
        "  - Categorical (nominal)\n",
        "\n",
        "2. How many null values? What percentage? What would you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No, the most common category makes up only 46.61% of the feature.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - There are 15 categories.No high cardinality ( > 10 categories)\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No."
      ],
      "metadata": {
        "id": "2pZRBfjqN83Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####relationship"
      ],
      "metadata": {
        "id": "lY4iNTcfOBn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = explore_categorical(df_adult, \"relationship\", figsize=(5,3))"
      ],
      "metadata": {
        "id": "ONVv1FAhOGgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations\n",
        "\n",
        "1. What type of feature is it? (Nominal category, ordinal, numeric)\n",
        "  - Categorical (nominal)\n",
        "\n",
        "2. How many null values? What percentage? What would you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No, the most common category makes up only 41.29% of the feature.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - There are 6 categories.No high cardinality ( < 10 categories)\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No."
      ],
      "metadata": {
        "id": "WEeD-PspOMb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####race"
      ],
      "metadata": {
        "id": "5MLCziWtOVNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = explore_categorical(df_adult, \"race\", figsize=(5,3))"
      ],
      "metadata": {
        "id": "C8Z-WTbDOciA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations\n",
        "\n",
        "1. What type of feature is it? (Nominal category, ordinal, numeric)\n",
        "  - Categorical (nominal)\n",
        "\n",
        "2. How many null values? What percentage? What would you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No, the most common category makes up only 85.68% of the feature.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - There are 5 categories.No high cardinality ( > 10 categories)\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No."
      ],
      "metadata": {
        "id": "EAQlvzUhOhSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####gender"
      ],
      "metadata": {
        "id": "DyeCPRbYOpZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = explore_categorical(df_adult, \"gender\", figsize=(5,3))"
      ],
      "metadata": {
        "id": "GfXvdQUPOzP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations\n",
        "\n",
        "1. What type of feature is it? (Nominal category, ordinal, numeric)\n",
        "  - Categorical (nominal)\n",
        "\n",
        "2. How many null values? What percentage? What would you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No, the most common category makes up only 67.59% of the feature.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - There are 2 categories.No high cardinality ( > 10 categories)\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No."
      ],
      "metadata": {
        "id": "GhYntUocPx0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Numeric"
      ],
      "metadata": {
        "id": "4jItBfhIP5GE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####age"
      ],
      "metadata": {
        "id": "9i4XeLCqMPYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the function\n",
        "explore_numeric(df_adult, \"age\");"
      ],
      "metadata": {
        "id": "eH1gx750r5xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "1. What type of feature is it? (categorical (nominal), ordinal, numeric?)\n",
        "  - Numeric.\n",
        "\n",
        "2. How many null values? What percentage? What will you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - Numeric feature, so not a concern.\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No"
      ],
      "metadata": {
        "id": "p1Lj7GGX3omg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = plot_categorical_vs_target(df_adult, 'age','income')"
      ],
      "metadata": {
        "id": "JCa-G7PYsbQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feature vs. Target Observations:\n",
        "  - Based on your business understanding, would you expect this feature to be a predictor of the target?\n",
        "    - Yes.\n",
        "  - Does this feature appear to be a predictor of the target?\n",
        "    - Yes, but there are some outleirs that may be affecting the overall trendline."
      ],
      "metadata": {
        "id": "B8_M4VSQ4c7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fnlwgt"
      ],
      "metadata": {
        "id": "024KNK3mQcNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the function\n",
        "explore_numeric(df_adult, \"fnlwgt\");"
      ],
      "metadata": {
        "id": "7aaUUJ-aQhgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "1. What type of feature is it? (categorical (nominal), ordinal, numeric?)\n",
        "  - Numeric.\n",
        "\n",
        "2. How many null values? What percentage? What will you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - Numeric feature, so not a concern.\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No"
      ],
      "metadata": {
        "id": "JJJwITx2QoSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = plot_categorical_vs_target(df_adult, 'fnlwgt','income')"
      ],
      "metadata": {
        "id": "BG6HDFdaQr4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feature vs. Target Observations:\n",
        "  - Based on your business understanding, would you expect this feature to be a predictor of the target?\n",
        "    - Yes.\n",
        "  - Does this feature appear to be a predictor of the target?\n",
        "    - Yes, but there are some outleirs that may be affecting the overall trendline."
      ],
      "metadata": {
        "id": "c3Wytj5gQxyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###educational-num"
      ],
      "metadata": {
        "id": "F_zCbvKoQ5jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the function\n",
        "explore_numeric(df_adult, \"educational-num\");"
      ],
      "metadata": {
        "id": "oelHmwBBQ-RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "1. What type of feature is it? (categorical (nominal), ordinal, numeric?)\n",
        "  - Numeric.\n",
        "\n",
        "2. How many null values? What percentage? What will you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - Numeric feature, so not a concern.\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No"
      ],
      "metadata": {
        "id": "mAikjhN1RDvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = plot_categorical_vs_target(df_adult, 'educational-num','income')"
      ],
      "metadata": {
        "id": "kQFVyJpwRIBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feature vs. Target Observations:\n",
        "  - Based on your business understanding, would you expect this feature to be a predictor of the target?\n",
        "    - Yes.\n",
        "  - Does this feature appear to be a predictor of the target?\n",
        "    - Yes, but there are some outleirs that may be affecting the overall trendline."
      ],
      "metadata": {
        "id": "TDU4BnPRROwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###capital-gain"
      ],
      "metadata": {
        "id": "IVIfpDczRT-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the function\n",
        "explore_numeric(df_adult, \"capital-gain\");"
      ],
      "metadata": {
        "id": "epIAUqP0RaQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "1. What type of feature is it? (categorical (nominal), ordinal, numeric?)\n",
        "  - Numeric.\n",
        "\n",
        "2. How many null values? What percentage? What will you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - Numeric feature, so not a concern.\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No"
      ],
      "metadata": {
        "id": "SFmNp94QRgUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = plot_categorical_vs_target(df_adult, 'capital-gain','income')"
      ],
      "metadata": {
        "id": "OTczI4UkRjTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feature vs. Target Observations:\n",
        "  - Based on your business understanding, would you expect this feature to be a predictor of the target?\n",
        "    - Yes.\n",
        "  - Does this feature appear to be a predictor of the target?\n",
        "    - Yes, but there are some outleirs that may be affecting the overall trendline."
      ],
      "metadata": {
        "id": "697anUZIRqEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###capital-loss"
      ],
      "metadata": {
        "id": "PZPyMTINRtlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the function\n",
        "explore_numeric(df_adult, \"capital-loss\");"
      ],
      "metadata": {
        "id": "cnNFiq0ERx_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "1. What type of feature is it? (categorical (nominal), ordinal, numeric?)\n",
        "  - Numeric.\n",
        "\n",
        "2. How many null values? What percentage? What will you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - Numeric feature, so not a concern.\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No"
      ],
      "metadata": {
        "id": "3zeJVkoPR3OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = plot_categorical_vs_target(df_adult, 'capital-loss','income')"
      ],
      "metadata": {
        "id": "ZR4V1vYwR7GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feature vs. Target Observations:\n",
        "  - Based on your business understanding, would you expect this feature to be a predictor of the target?\n",
        "    - Yes.\n",
        "  - Does this feature appear to be a predictor of the target?\n",
        "    - Yes, but there are some outleirs that may be affecting the overall trendline."
      ],
      "metadata": {
        "id": "UqYWEwjqSC6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###hours-per-week"
      ],
      "metadata": {
        "id": "YWgmAYYSSIyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the function\n",
        "explore_numeric(df_adult, \"hours-per-week\");"
      ],
      "metadata": {
        "id": "Cmt3R1atSNcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "1. What type of feature is it? (categorical (nominal), ordinal, numeric?)\n",
        "  - Numeric.\n",
        "\n",
        "2. How many null values? What percentage? What will you do with the null values (drop the rows? drop the column? impute? if impute, with what?)\n",
        "  - 0 null values.\n",
        "  - No need to impute.\n",
        "3. Is the feature constant or quasi-constant?\n",
        "  - No.\n",
        "\n",
        "4. What is the cardinality? Is it high?\n",
        "  - Numeric feature, so not a concern.\n",
        "\n",
        "5. Would we know this BEFORE the target is determined?\n",
        "  - Yes.\n",
        "\n",
        "6. Is there a business case/understanding reason to exclude based on our business case?\n",
        "  - No"
      ],
      "metadata": {
        "id": "wpTUWTgLSTOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "fig, ax = plot_categorical_vs_target(df_adult, 'hours-per-week','income')"
      ],
      "metadata": {
        "id": "UEZwp9nFSWSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feature vs. Target Observations:\n",
        "  - Based on your business understanding, would you expect this feature to be a predictor of the target?\n",
        "    - Yes.\n",
        "  - Does this feature appear to be a predictor of the target?\n",
        "    - Yes, but there are some outleirs that may be affecting the overall trendline."
      ],
      "metadata": {
        "id": "QKVZPE34ScsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Produced univariate visuals for the target and all features."
      ],
      "metadata": {
        "id": "4b9CjS3K5KX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the counts for the number of adults who has income vs not having income.\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "survived_counts = df_adult['income'].value_counts()\n",
        "plt.bar(['non-income', 'income'], survived_counts)\n",
        "#plt.bar(['Died', 'Survived'], survived_counts)\n",
        "\n",
        "# Set x-label, y-label, and title\n",
        "plt.xlabel('income Status')\n",
        "plt.ylabel('Count')\n",
        "plt.title('non-income vs income')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyHgMLoz306J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot a histogram of the distribution of Age.\n",
        "ax = df_adult['age'].hist(bins = 30, edgecolor = 'black')\n",
        "ax.tick_params(axis='x', rotation = 45)\n",
        "ax.ticklabel_format(style='plain')\n",
        "ax.set_title('Distribution of Age')\n",
        "ax.set_xlabel('Age')\n",
        "ax.set_ylabel('Number age group')"
      ],
      "metadata": {
        "id": "MS1OBg_H2Eyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the boxplot\n",
        "ax = sns.boxplot(data=df_adult,x=\"hours-per-week\")\n",
        "ax.set_xlabel(\"group\");\n",
        "ax.set_ylabel(\"hours-per-week\");\n",
        "ax.set_title(\"Total working hours\");"
      ],
      "metadata": {
        "id": "ElmLlYqG3eIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####choose two visualizations from your analysis"
      ],
      "metadata": {
        "id": "PNxoKUBUHub_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####histogram"
      ],
      "metadata": {
        "id": "637EHFrP6ivk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the histogram\n",
        "ax = df_adult['income'].hist(bins =20, edgecolor = 'black')\n",
        "ax.set_title('Distribution of Income')\n",
        "ax.set_xlabel('Income')\n",
        "ax.set_ylabel('Number of Individuals');"
      ],
      "metadata": {
        "id": "Bz2M1tG451xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Individuals with significant capital gains could be a potential target for higher-income financial products.\n",
        "- They might consider targeting individuals in the age range where the probability of earning >50K is higher, or focus on promoting higher education to improve the chances of higher income."
      ],
      "metadata": {
        "id": "2crp0wV1Ksfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####boxplot"
      ],
      "metadata": {
        "id": "hAYdRxCz6dhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the boxplot\n",
        "ax = sns.boxplot(x = 'hours-per-week', y = 'income', data = df_adult)\n",
        "ax.set_title('INCOME by work-hours');"
      ],
      "metadata": {
        "id": "164jBzUn6AAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Individuals earning >50K tend to work slightly more hours per week on average compared to those earning <=50K. The distribution of hours worked per week is more spread out for the >50K group."
      ],
      "metadata": {
        "id": "wbwx7akPLCzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Plot a corrleation"
      ],
      "metadata": {
        "id": "2jMEbdJd9nXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(28, 18))\n",
        "ax = sns.countplot(data=df_adult, hue='income', x='occupation');\n",
        "ax.set(title='# of income - by occupation');"
      ],
      "metadata": {
        "id": "5b6PSivU90Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Individuals with higher educational-num tend to have a higher likelihood of earning >50K. The distribution of educational-num is shifted to the right for individuals earning >50K, indicating a higher educational level."
      ],
      "metadata": {
        "id": "HnLCP0d6LNqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multivariate Visual"
      ],
      "metadata": {
        "id": "0r8YzxYC4lvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) Plot the counts for the number of income by Sex"
      ],
      "metadata": {
        "id": "IQ4wqa9f5Hhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.countplot(data=df_adult, hue='gender', x='income');\n",
        "ax.set(title='# of Income - by Gender');"
      ],
      "metadata": {
        "id": "yN_rCgP_4xND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "​Question: Which group had a higher income ?\n",
        "- Answer: Male has higher income."
      ],
      "metadata": {
        "id": "bBYi1KmFycPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Plot a barplot of the average"
      ],
      "metadata": {
        "id": "U24f5Fky2q-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## functionize a barplot and call it plot_bar\n",
        "\n",
        "def plot_bar(data,x, y, figsize=(20,5)):\n",
        "    ## Make a larger fig/ax before plotting\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    ## Plot barplot\n",
        "\n",
        "    ## counts, counts up the categories, sort the values, then puts them in a\n",
        "    ## data frame.\n",
        "\n",
        "    counts = df_adult[x].value_counts().sort_values(ascending=False).to_frame()\n",
        "    ## ploting the barplot\n",
        "\n",
        "    sns.barplot(data=counts,x=counts.index, y = counts[x],ax=ax)\n",
        "\n",
        "    plt.xticks(rotation= 90)\n",
        "    ax.set_title(f'Categorical Frequencies of {y}', fontsize = 20, fontweight = 'bold');\n",
        "    ax.set_xlabel(f'Categories for {y}', fontsize = 15, fontweight = 'bold')\n",
        "    ax.set_ylabel('Counts', fontsize = 15, fontweight = 'bold')\n",
        "\n",
        "    return fig,ax"
      ],
      "metadata": {
        "id": "oWEeFIIJxZ2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plot_bar(df_adult, 'income', 'Experience income');\n",
        "print(df_adult['income'].value_counts())"
      ],
      "metadata": {
        "id": "hEgIqKBNxhL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Majority of people got <=50k income  "
      ],
      "metadata": {
        "id": "E5Q86F6B0Ho3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.barplot(data=df_adult,x='income', y='hours-per-week')\n",
        "ax.set_title(\"Earnings based on hours of wrok\");"
      ],
      "metadata": {
        "id": "UcU5xmx02ACV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- people who worked more than 40 hours, got higher salary  "
      ],
      "metadata": {
        "id": "COsXmTVPyqO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Plot a corrleation heatmap of all of the numeric columns."
      ],
      "metadata": {
        "id": "gi1-86FT22m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping dictionary\n",
        "income_mapping = {'<=50K': 0, '>50K': 1}\n",
        "\n",
        "# Create a new DataFrame with the transformed 'income' column\n",
        "df_transformed = df_adult.copy()  # Make a copy of the original DataFrame\n",
        "df_transformed['income'] = df_adult['income'].map(income_mapping)"
      ],
      "metadata": {
        "id": "BAejN6oDgoPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df_transformed.corr(numeric_only=True)\n",
        "ax = sns.heatmap(corr,annot=True, cmap='coolwarm');\n",
        "ax.set(title='Correlation Heatmap')"
      ],
      "metadata": {
        "id": "mM-9mIgM1N5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: which column had the highest positive correlation to the \"Income\" column?\n",
        "- Answer: the income column was most positively correlated with the hours-per-week column."
      ],
      "metadata": {
        "id": "u184f-RIzop3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Modeling"
      ],
      "metadata": {
        "id": "-V3PnoLo2Y_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Check Class Balance**"
      ],
      "metadata": {
        "id": "iyDJPXo1H6lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_adult['income'].value_counts(normalize = False)"
      ],
      "metadata": {
        "id": "ke-RHdEPH9s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many samples of each class are present\n",
        "df_adult['income'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "atWmkEIjIC2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Split the Data**"
      ],
      "metadata": {
        "id": "FLzN3tJEIMdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X and y\n",
        "target = 'income'\n",
        "X = df_adult.drop(columns = [target]).copy()\n",
        "y = df_adult[target]"
      ],
      "metadata": {
        "id": "7X6l29LxIPl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test-split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "2douMbFnIU0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Class Balance"
      ],
      "metadata": {
        "id": "dJdyFNdNIZqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many samples of each class are present for train\n",
        "y_train.value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "lKFOScx5Ic7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transformers**"
      ],
      "metadata": {
        "id": "ouYkZlI6IgQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical Preprocessing Pipeline\n",
        "# Save list of column names\n",
        "ohe_cols = X_train.select_dtypes('object').columns\n",
        "print(\"OneHotEncoder Columns:\", ohe_cols)\n",
        "# Instantiate the individual preprocessors\n",
        "impute_na = SimpleImputer(strategy='constant', fill_value = \"Missing\")\n",
        "ohe_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "# Make pipeline with imputer and encoder\n",
        "ohe_pipe = make_pipeline(impute_na, ohe_encoder)\n",
        "# Making a ohe_tuple for ColumnTransformer\n",
        "ohe_tuple = ('categorical', ohe_pipe, ohe_cols)\n",
        "# Numerical Preprocessing Pipeline\n",
        "# Save list of column names\n",
        "num_cols = X_train.select_dtypes(\"number\").columns\n",
        "print(\"Numeric Columns:\", num_cols)\n",
        "# instantiate preprocessors\n",
        "impute_median = SimpleImputer(strategy='median')\n",
        "scaler = StandardScaler()\n",
        "# Make a numeric preprocessing pipeline\n",
        "num_pipe = make_pipeline(impute_median, scaler)\n",
        "# Making a numeric tuple for ColumnTransformer\n",
        "num_tuple = ('numeric', num_pipe, num_cols)\n",
        "# Create the Column Transformer\n",
        "preprocessor = ColumnTransformer([num_tuple, ohe_tuple],\n",
        "                                 verbose_feature_names_out=False)"
      ],
      "metadata": {
        "id": "ul8Wx0R-IjXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation Functions"
      ],
      "metadata": {
        "id": "f7cOSk47I0d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classification_metrics(y_true, y_pred, label='',\n",
        "                           output_dict=False, figsize=(8,4),\n",
        "                           normalize='true', cmap='Blues',\n",
        "                           colorbar=False):\n",
        "  # Get the classification report\n",
        "  report = classification_report(y_true, y_pred)\n",
        "  ## Print header and report\n",
        "  header = \"-\"*70\n",
        "  print(header, f\" Classification Metrics: {label}\", header, sep='\\n')\n",
        "  print(report)\n",
        "  ## CONFUSION MATRICES SUBPLOTS\n",
        "  fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
        "  # create a confusion matrix  of raw counts\n",
        "  ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
        "                normalize=None, cmap='gist_gray', colorbar=colorbar,\n",
        "                ax = axes[0],);\n",
        "  axes[0].set_title(\"Raw Counts\")\n",
        "  # create a confusion matrix with the test data\n",
        "  ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
        "                normalize=normalize, cmap=cmap, colorbar=colorbar,\n",
        "                ax = axes[1]);\n",
        "  axes[1].set_title(\"Normalized Confusion Matrix\")\n",
        "  # Adjust layout and show figure\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "  # Return dictionary of classification_report\n",
        "  if output_dict==True:\n",
        "    report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
        "    return report_dict\n",
        "\n",
        "def evaluate_classification(model, X_train, y_train, X_test, y_test,\n",
        "                            figsize=(6,4), normalize='true', output_dict = False,\n",
        "                            cmap_train='Blues', cmap_test=\"Reds\",colorbar=False):\n",
        "  # Get predictions for training data\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  # Call the helper function to obtain regression metrics for training data\n",
        "  results_train = classification_metrics(y_train, y_train_pred, #verbose = verbose,\n",
        "                                         output_dict=True, figsize=figsize,\n",
        "                                         colorbar=colorbar, cmap=cmap_train,\n",
        "                                         label='Training Data')\n",
        "  print()\n",
        "  # Get predictions for test data\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  # Call the helper function to obtain regression metrics for test data\n",
        "  results_test = classification_metrics(y_test, y_test_pred, #verbose = verbose,\n",
        "                                        output_dict=True,figsize=figsize,\n",
        "                                        colorbar=colorbar, cmap=cmap_test,\n",
        "                                        label='Test Data' )\n",
        "  if output_dict == True:\n",
        "    # Store results in a dataframe if ouput_frame is True\n",
        "    results_dict = {'train':results_train,\n",
        "                    'test': results_test}\n",
        "    return results_dict"
      ],
      "metadata": {
        "id": "p0n8E9HCI6_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Model - 1"
      ],
      "metadata": {
        "id": "Rzj6Wm1dHTg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "iT6RPGRoJkk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the model\n",
        "dtc = DecisionTreeClassifier(random_state=42)\n",
        "# Create a model pipeline\n",
        "clf_pipe = make_pipeline(preprocessor, dtc)\n",
        "# Fit the model\n",
        "clf_pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3L0kSkoSIpGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_pipe.predict(X_train)\n"
      ],
      "metadata": {
        "id": "gt2LyYAuIs4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "h4IV1TbKIvkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluate the model\n",
        "evaluate_classification(clf_pipe, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "bPKH1DzFJEWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The model appears to be performing very well on the training data with perfect precision, recall, and F1-score.\n",
        "- However, on the test data, the performance is slightly lower, indicating that the model might be overfitting to the training data."
      ],
      "metadata": {
        "id": "Vqxywa2t498g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using GridSearch CV"
      ],
      "metadata": {
        "id": "xH5GIQzpLrQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'decisiontreeclassifier__max_depth': [None, 10, 20],\n",
        "    'decisiontreeclassifier__min_samples_split': [2, 5, 10],\n",
        "    'decisiontreeclassifier__min_samples_leaf': [1, 2, 4]\n",
        "}"
      ],
      "metadata": {
        "id": "iG72GAAMLvRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create GridSearchCV instance\n",
        "grid_search = GridSearchCV(clf_pipe, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n"
      ],
      "metadata": {
        "id": "tQeBhkXvMwDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalaute the best model\n",
        "best_logreg = grid_search.best_estimator_\n",
        "evaluate_classification(best_logreg, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "f7OkqO5FM4BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Applying GridSearchCV has led to improvements in the model's performance on both the training and test data. The model now achieves higher precision, recall, and F1-score for both classes.\n",
        "- The accuracy has also improved, indicating that the model's generalization capability has improved as well."
      ],
      "metadata": {
        "id": "cd82Wrun5Nct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using PCA"
      ],
      "metadata": {
        "id": "IjzsmxXbBRrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transformer pipeline\n",
        "transformer_pca_dcn = make_pipeline(preprocessor, PCA(n_components=7))"
      ],
      "metadata": {
        "id": "pZRooIRZBVJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a modeling pipeline\n",
        "dtc_pipe_pca = make_pipeline(transformer_pca_dcn, dtc)\n",
        "dtc_pipe_pca.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "DC6cnsddBisk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtc_pipe_pca.predict(X_train)"
      ],
      "metadata": {
        "id": "0reZoYQ4B45G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluate the model\n",
        "evaluate_classification(dtc_pipe_pca, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "bMxGwSQyCE94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Using PCA with a DecisionTreeClassifier has resulted in a model with perfect performance on the training data, as indicated by precision, recall, and F1-score values of 1.00 for both classes.\n",
        "- This suggests that the model has potentially overfit the training data, given the perfect scores.\n",
        "- However, the model's performance on the test data is not as strong. While the model is still accurate, the precision, recall, and F1-score values for the '>50K' class are lower than those for the '<=50K' class.\n",
        "- This indicates that the model's ability to predict the '>50K' class is not as reliable as its ability to predict the '<=50K' class"
      ],
      "metadata": {
        "id": "lJIMBAUqCkGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model -2"
      ],
      "metadata": {
        "id": "Me9Ju2rfJO3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Logistic Regression"
      ],
      "metadata": {
        "id": "_TCe62mrJfL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make an instance of the model with default parameters\n",
        "logreg = LogisticRegression(max_iter=500, random_state=42)\n",
        "# Put scaler and model in a pipeline\n",
        "logreg_pipe = make_pipeline(preprocessor, logreg)\n",
        "# Training the model on the data, storing the information learned from the data\n",
        "# Model is learning the relationship between X and y\n",
        "logreg_pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "qn7mZJ1cJpdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_pipe.predict(X_train)"
      ],
      "metadata": {
        "id": "5ximefbOJuGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "UgLhwpcaJywc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluate the model\n",
        "evaluate_classification(logreg_pipe, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "bbyFMgVEJ3NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Logistic Regression model seems to be performing consistently on both the training and test data.\n",
        "- The precision, recall, and F1-score values for both classes are similar between the training and test sets, indicating that the model's performance is stable across different data subsets."
      ],
      "metadata": {
        "id": "c-dRKfXK5rwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using GridSearch CV"
      ],
      "metadata": {
        "id": "mmRdXpsfKBLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a logreg pipe that uses L2 regularization\n",
        "logreg_pipe = make_pipeline(preprocessor, LogisticRegression(solver='liblinear',\n",
        "                                                       max_iter=500,\n",
        "                                                       penalty='l2',\n",
        "                                                       random_state=42))\n",
        "# Define the params and instantiate gridsearch\n",
        "l2_params = {'logisticregression__solver': ['lbfgs', 'liblinear', 'newton-cg',\n",
        "                                            'newton-cholesky', 'sag', 'saga'],\n",
        "                  'logisticregression__penalty' : ['l2'],\n",
        "                  'logisticregression__C': [0.0001, 0.001, 0.01, 0.1,\n",
        "                                            1, 10, 100, 1000,10000] }\n",
        "gs = GridSearchCV(logreg_pipe, l2_params,verbose=True, n_jobs=-1,\n",
        "                  return_train_score=True,scoring='recall_macro')\n",
        "# Fit gridsearch and display best params\n",
        "gs.fit(X_train, y_train)\n",
        "gs.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfxONO4pKFUW",
        "outputId": "c59487c7-9172-4121-dd36-577ba85add4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalaute the best model\n",
        "best_logreg = gs.best_estimator_\n",
        "evaluate_classification(best_logreg, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "eIWHaffJKLS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The precision, recall, and F1-score values are similar between the two datasets, indicating that the model is not overfitting.\n",
        "- An accuracy of 0.85 suggests that the model's predictions are relatively accurate, though there's a trade-off between precision and recall, particularly for the '>50K' class."
      ],
      "metadata": {
        "id": "16iBMUGl6BXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Create a PCA Pipeline with Regression"
      ],
      "metadata": {
        "id": "PJuEyHuz-Blb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transformer pipeline\n",
        "transformer_pca = make_pipeline(preprocessor, PCA(n_components=7))"
      ],
      "metadata": {
        "id": "aOUuSJGY-QfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a modeling pipeline\n",
        "logreg_pipe_pca = make_pipeline(transformer_pca, logreg)\n",
        "logreg_pipe_pca.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "pXHY1kzJ-feS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_pipe_pca.predict(X_train)"
      ],
      "metadata": {
        "id": "5KSp8xNIALgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluate the model\n",
        "evaluate_classification(logreg_pipe_pca, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "xnvHzoD8AXVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The use of PCA along with Regression has resulted in a model with slightly lower accuracy compared to the previous models.\n",
        "- The precision, recall, and F1-score values are consistent across training and test datasets, indicating that the model generalizes well to new data.\n",
        "- The model seems to be better at predicting the '<=50K' class, as indicated by higher precision and recall values for that class.\n",
        "-  However, the model's performance on the '>50K' class is relatively weaker, with lower precision and recall."
      ],
      "metadata": {
        "id": "9BLQXlykA_H1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Choose your \"production\" model and justify this decision using the metrics most important to your business problem."
      ],
      "metadata": {
        "id": "w9zh9XX2ER6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. DecisionTreeClassifier:\n",
        "\n",
        "   - Training Data Accuracy: 100%\n",
        "   - Test Data Accuracy: 81%\n",
        "\n",
        "2. GridSearchCV (with DecisionTreeClassifier):\n",
        "   - Training Data Accuracy: 87%\n",
        "   - Test Data Accuracy: 85%\n",
        "\n",
        "3. PCA with DecisionTreeClassifier:\n",
        "  - Training Data Accuracy: 100%\n",
        "  - Test Data Accuracy: 78%\n",
        "\n",
        "4. LogisticRegression:\n",
        "\n",
        "  - Training Data Accuracy: 85%\n",
        "  - Test Data Accuracy: 85%\n",
        "\n",
        "5. PCA (with Regression):\n",
        "  - Training Data Accuracy: 83%\n",
        "  - Test Data Accuracy: 84%\n",
        "\n",
        "- Based on the accuracy scores, the DecisionTreeClassifier achieves the highest accuracy on the training data, but it has the lowest accuracy on the test data, suggesting potential overfitting.\n",
        "- The PCA with DecisionTreeClassifier also shows signs of overfitting, as it performs perfectly on the training data but significantly worse on the test data.\n",
        "- Among the remaining models, both the GridSearchCV model with DecisionTreeClassifier and the LogisticRegression model have similar accuracy scores on both the training and test data. These two models generalize well and have a balance between precision and recall for both classes."
      ],
      "metadata": {
        "id": "ZgBicL5GEY8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### which model ?"
      ],
      "metadata": {
        "id": "egg3O-NtFghS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Given that the primary business goal is likely to have a model that generalizes well to new, unseen data, the production model choice should be based on the balance between performance on the test data and avoiding overfitting. In this case, the LogisticRegression model would be a suitable choice for production."
      ],
      "metadata": {
        "id": "p6j41dFGFZxg"
      }
    }
  ]
}